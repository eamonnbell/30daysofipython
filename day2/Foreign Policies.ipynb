{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2: Foreign Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I thought it would be a nice idea to look at some of U.S. President Obama's speeches and mine them, using `word2vec`. \n",
    "\n",
    "Inspired by [this blog post](http://byterot.blogspot.com/2015/06/five-crazy-abstractions-my-deep-learning-word2doc-model-just-did-NLP-gensim.html) by aliostad. \n",
    "\n",
    "The first thing to do with any text analysis project is to get the corpus. The speech texts I used were found [here](http://obamaspeeches.com/). `jusText` [(github)](https://github.com/miso-belica/jusText) was used with some success to remove boilerplate text from the website. I download the speeches, clean them up a bit, tokenize them with `spaCy`, map them onto a vector space with `word2vec` ([github](https://github.com/miso-belica/jusText); a Python module which wraps C binaries that implement the word2vec algorithm) trained on the corpus. I used a list of countries I found online to find out which countries were mentioned most and then used the convenience functions in `word2vec` to list the top three tokens ranked according to their cosine similarity in the model's space. Results below. I would have utilized `spaCy` and/or `gensim` (both great) a bit more agressively if I had the time. Perhaps something worth returning to.\n",
    "\n",
    "Example results:\n",
    "```\n",
    "UKRAINE ~ REGION, COURSE, NUMBER\n",
    "AFGHANISTAN ~ UNION, U.S., BETWEEN\n",
    "ISRAEL ~ TEST, AMENDMENT, AROUND\n",
    "GEORGIA ~ PROCESS, BUILT, BUILDING\n",
    "IRAN ~ REFORM, LOCAL, INCREASE\n",
    "INDIA ~ THOUSANDS, STUDENTS, EFFORTS\n",
    "CHINA ~ SERIES, CITIZENSHIP, TERRORIST\n",
    "KENYA ~ CHICAGO, DURING, MAN\n",
    "IRAQ ~ WAR, PRESIDENT, IN\n",
    "RUSSIA ~ HEART, OPEN, STEM\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "import json\n",
    "import justext\n",
    "import requests\n",
    "import word2vec\n",
    "import spacy.en\n",
    "import collections\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the parser and prepare token probabilities for stopword filtering\n",
    "\n",
    "nlp = spacy.en.English()\n",
    "probs = [lex.prob for lex in nlp.vocab]\n",
    "probs.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('urllist.txt') as f:\n",
    "    url_list = f.readlines()\n",
    "    url_list = [url.strip() for url in url_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_corpus_from_url_list(url_list):\n",
    "    \"\"\"Generates a list of documents stripped of boilerplate text.\"\"\"\n",
    "    corpus = []\n",
    "    \n",
    "    for url in url_list:\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Remove boilerplate with jusText\n",
    "        paragraphs = justext.justext(response.content, justext.get_stoplist(\"English\"))\n",
    "        content_paragraphs = []\n",
    "\n",
    "        for paragraph in paragraphs[:-1]:\n",
    "            if not paragraph.is_boilerplate:\n",
    "                content_paragraphs.append(paragraph.text)\n",
    "        \n",
    "        speech = \"\\n\".join(content_paragraphs)\n",
    "        corpus.append(speech)\n",
    "    \n",
    "    return corpus            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "big_corpus = get_corpus_from_url_list(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convenience functions to save from downloading the corpus over an over\n",
    "\n",
    "def save_corpus():\n",
    "    with open('corpus.json', 'w') as fp:\n",
    "        json.dump(big_corpus, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_corpus():\n",
    "    with open('corpus.json', 'r') as fp:\n",
    "        big_corpus = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lump the whole corpus together; not interested in document-level stats\n",
    "bundle = \"\".join(big_corpus)\n",
    "bundle = bundle.replace('\\n', '')\n",
    "bundle = bundle.replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize with spaCy\n",
    "\n",
    "tokens = nlp(bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Filter common words (p < that of top 100 in spaCy's model)\n",
    "\n",
    "cleaned_tokens = [tok for tok in tokens if tok.prob < probs[-100]]\n",
    "cleaned_tokens_strings = [tok.string.upper().strip() for tok in cleaned_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/kernel/__main__.py:5: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "with open('countries.txt', 'r') as fp:\n",
    "    countries = fp.readlines()\n",
    "    countries = [line.strip() for line in countries]\n",
    "    \n",
    "mentioned_countries = [tok for tok in cleaned_tokens_strings if tok in countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'AFGHANISTAN': 31,\n",
       " u'CHINA': 31,\n",
       " u'GEORGIA': 11,\n",
       " u'INDIA': 14,\n",
       " u'IRAN': 23,\n",
       " u'IRAQ': 286,\n",
       " u'ISRAEL': 16,\n",
       " u'KENYA': 46,\n",
       " u'RUSSIA': 29,\n",
       " u'UKRAINE': 11}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_counter = collections.Counter(mentioned_countries)\n",
    "common_countries= dict(country_counter.most_common(10))\n",
    "common_countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'word2phrase', u'-train', u'corpus.txt', u'-output', u'corpus-phrases.txt', u'-min-count', u'5', u'-threshold', u'100', u'-debug', u'2']\n"
     ]
    }
   ],
   "source": [
    "cleaned_corpus = \" \".join(tok.string.upper() for tok in cleaned_tokens)\n",
    "\n",
    "# Write out the cleaned corpus into a text file for use by word2vec\n",
    "\n",
    "with open('corpus.txt', 'wb') as f:\n",
    "    f.write(cleaned_corpus)\n",
    "\n",
    "# Chunk common stuff into a phrase (see word2vec docs)\n",
    "\n",
    "word2vec.word2phrase('corpus.txt', 'corpus-phrases.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model and save to corpus.bin\n",
    "\n",
    "word2vec.word2vec('corpus.txt', 'corpus.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3523, (3523, 100))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load up the model\n",
    "\n",
    "model = word2vec.load('corpus.bin')\n",
    "\n",
    "# The model has a default vector size of 100; can be tweaked for performance\n",
    "# This corpus has 3525 words\n",
    "\n",
    "len(model.vocab), model.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def top_n(indexes, metrics, n=3):\n",
    "    top_n = model.generate_response(indexes, metrics).tolist()[:n]\n",
    "    return [x for x in top_n]\n",
    "\n",
    "def closest_to(token):\n",
    "    \"\"\"Gets words near each other in the space using cosine similarity.\n",
    "    Gives its results in a tuple which indexes the words in the corpus.\n",
    "    So it's passed to top_n() which just prettifies and gets the top n.\"\"\"\n",
    "    indexes, metrics = model.cosine(token)\n",
    "    return top_n(indexes, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UKRAINE ~ REGION, NUMBER, EFFORTS\n",
      "AFGHANISTAN ~ UNION, U.S., BETWEEN\n",
      "ISRAEL ~ TEST, AROUND, PERSONAL\n",
      "GEORGIA ~ PROCESS, PROCEDURES, BUILT\n",
      "IRAN ~ REFORM, INCREASE, LOCAL\n",
      "INDIA ~ STUDENTS, HOUR, THOUSANDS\n",
      "CHINA ~ SERIES, TERRORIST, LAND\n",
      "KENYA ~ CHICAGO, FOUND, BEING\n",
      "IRAQ ~ WAR, PRESIDENT, IN\n",
      "RUSSIA ~ HEART, LEGAL, EFFORT\n"
     ]
    }
   ],
   "source": [
    "for country in common_countries.keys():\n",
    "    closest = closest_to(country)\n",
    "    pretty = \", \".join([x[0] for x in closest])\n",
    "    print \"{} ~ {}\".format(country, pretty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "country_pairings = list(itertools.combinations(common_countries.keys(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UKRAINE + AFGHANISTAN ~ UNION, U.S., IRAQI\n",
      "UKRAINE + ISRAEL ~ REGION, COURSE, USED\n",
      "UKRAINE + GEORGIA ~ COURSE, LEADERSHIP, SOLDIERS\n",
      "UKRAINE + IRAN ~ REGION, EFFORTS, LOCAL\n",
      "UKRAINE + INDIA ~ EFFORTS, STUDENTS, DANGEROUS\n",
      "UKRAINE + CHINA ~ REGION, EFFORTS, INTELLIGENCE\n",
      "UKRAINE + KENYA ~ NUMBER, BOTH, SUCH\n",
      "UKRAINE + IRAQ ~ AGAINST, UNION, WAR\n",
      "UKRAINE + RUSSIA ~ EFFORTS, REGION, NUMBER\n",
      "AFGHANISTAN + ISRAEL ~ ECONOMIC, SUCH, REGION\n",
      "AFGHANISTAN + GEORGIA ~ AMENDMENT, WITHIN, LEADERSHIP\n",
      "AFGHANISTAN + IRAN ~ EFFORTS, SUCH, LED\n",
      "AFGHANISTAN + INDIA ~ EFFORTS, STUDENTS, DANGEROUS\n",
      "AFGHANISTAN + CHINA ~ LED, EFFORTS, SUCH\n",
      "AFGHANISTAN + KENYA ~ BOTH, STATE, SUCH\n",
      "AFGHANISTAN + IRAQ ~ WAR, AGAINST, CIVIL\n",
      "AFGHANISTAN + RUSSIA ~ EFFORTS, SUCH, U.S.\n",
      "ISRAEL + GEORGIA ~ TEST, AS, PROCESS\n",
      "ISRAEL + IRAN ~ ABILITY, AROUND, FULL\n",
      "ISRAEL + INDIA ~ FULL, STUDENTS, RESEARCH\n",
      "ISRAEL + CHINA ~ TEST, HEART, AMENDMENT\n",
      "ISRAEL + KENYA ~ HEART, HE, BEING\n",
      "ISRAEL + IRAQ ~ U.S., UNION, AGAINST\n",
      "ISRAEL + RUSSIA ~ HEART, AMENDMENT, TEST\n",
      "GEORGIA + IRAN ~ ENTIRE, AMENDMENT, DEMOCRACY\n",
      "GEORGIA + INDIA ~ ENTIRE, CASE, AMENDMENT\n",
      "GEORGIA + CHINA ~ GOES, TEST, AMENDMENT\n",
      "GEORGIA + KENYA ~ AS, BUILT, TERMS\n",
      "GEORGIA + IRAQ ~ UNION, FORCES, U.S.\n",
      "GEORGIA + RUSSIA ~ DEMOCRACY, AMENDMENT, AS\n",
      "IRAN + INDIA ~ STUDENTS, THOUSANDS, EFFORTS\n",
      "IRAN + CHINA ~ TERRORIST, LEGAL, THOUSANDS\n",
      "IRAN + KENYA ~ SUCH, RUSSIA, BECOME\n",
      "IRAN + IRAQ ~ AGAINST, U.S., UNION\n",
      "IRAN + RUSSIA ~ REFORM, LEGAL, EFFORT\n",
      "INDIA + CHINA ~ STUDENTS, THOUSANDS, LEGAL\n",
      "INDIA + KENYA ~ FOR, AFRICA, AMONG\n",
      "INDIA + IRAQ ~ U.S., UNION, WHITE\n",
      "INDIA + RUSSIA ~ STUDENTS, LEGAL, EFFORT\n",
      "CHINA + KENYA ~ HEART, AFRICA, FOUND\n",
      "CHINA + IRAQ ~ U.S., AGAINST, UNION\n",
      "CHINA + RUSSIA ~ HEART, LEGAL, LAND\n",
      "KENYA + IRAQ ~ SENATE, OTHER, U.S.\n",
      "KENYA + RUSSIA ~ BEING, FOUND, AFRICA\n",
      "IRAQ + RUSSIA ~ UNION, U.S., AGAINST\n"
     ]
    }
   ],
   "source": [
    "# word2vec has the concept of an analogy\n",
    "\n",
    "for x, y in country_pairings:\n",
    "    indexes, metrics = model.analogy(pos=[x, y], neg=[], n=10)\n",
    "    analogy = top_n(indexes, metrics)\n",
    "    pretty = \", \".join([i[0] for i in analogy])\n",
    "    print \"{} + {} ~ {}\".format(x, y, pretty)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
